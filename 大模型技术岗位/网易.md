# 网易

创建时间: 2025年5月14日 00:22

# 面试题目

```markdown
1. 请对比分析对比损失在负样本只有一个时与交叉熵的等价性
2. 如何处理长尾分布问题？
3. RAG记忆模块的平滑设计是如何实现的？
4. 请解释Transformer-XL中绝对形式相对位置编码的原理
5. 请手写实现多头注意力(MHA)和RoPE(旋转位置编码)
6. 你期望从事哪方面的工作方向？
7. 推理优化主要支撑哪些业务场景？
8. 引擎后续的工作规划是怎样的？
9. 请介绍你参与的RAG项目，团队分工是怎样的？
10. 在RAG项目中哪种优化方式对指标提升最明显？
11. 你对Triton了解多少？
12. 如何为多头注意力添加RoPE？
13. 使用交叉熵作为评估指标可能有哪些问题和优势？
14. 请介绍flash attention和paged attention的原理
15. 不带mask的多头注意力层有什么特点？
16. 多标签意图识别任务应该如何设计？
17. 请解释VLLM推理的原理
18. 如何找出字符串中出现最多次数的字符？
19. 如何找出不含重复字符的最长子串？
20. 请解释什么是MLA(Multi-Query Attention)？
21. 大模型在代码生成任务中的效果如何评估？
22. 训练时为什么要使用mask？推理时需要吗？两者有什么区别？
23. 训练和推理时的sequence length是否相同？
24. 请介绍非线性长度外推方法
25. 请介绍你在智能助手项目的实习经历
26. 在SFT(监督微调)中哪部分会计算损失？为什么？
27. 训练模型时使用了几张GPU卡？遇到过训练中断问题吗？
28. 请介绍LLAMA模型的结构特点
29. 为什么算法题要用C++而不是Python实现？
30. 什么是混合精度训练？
31. 项目中使用的强化学习模型是哪种？
32. LLM Agent与RL Agent有什么区别？
33. Transformer和传统seq2seq模型的主要区别是什么？
34. 情绪流建模为什么是必要的？
35. 面试中问了很多C++基础知识，包括继承、多态、const和static等
36. 请介绍实习期间的意图识别工作
37. 训练或微调一个LLM的基本流程是什么？
38. MLM(掩码语言模型)和NSP(下一句预测)有哪些缺点？
39. LLaMA中使用的是哪种attention变体？
40. 意图识别使用对比学习损失与文本分类使用交叉熵有什么区别？
41. LORA为什么训练速度快？显存中主要存放什么？
42. 如何将你开发的项目系统引入搜索引擎？
43. 请解释数值上溢和下溢问题
44. 请介绍你喜欢的游戏
45. 编写代码：反转字符串中的元音字母
46. 项目中强化学习的奖励模型是如何设计的？
47. 请写出attention公式并用代码实现
48. SFT使用的损失函数是什么？
49. attention计算时有哪些scale处理方式？
50. 如何向小学生解释数据库？数据库与Excel有什么区别？
51. 请详细介绍项目流程，每一步做了什么？项目具体排名如何？
52. 训练一个7B参数模型需要多少显存？不同Zero阶段能节省多少显存？
53. 请介绍BERT的预训练方法
54. 请介绍VIT(Vision Transformer)
55. 请解释思维链(Chain-of-Thought)及其研究现状
56. BERT和MBERT的区别是什么？
57. 请介绍Transformer及多头注意力机制，为什么要采用多头设计？
58. Transformer-XL与RoPE有什么异同？
59. 请介绍PPO算法
60. 请介绍量化算法的原理
61. 了解哪些加速引擎？它们如何提升运算速度？
62. 了解哪些attention的变种？
63. 实习时是否使用过Megatron框架？对分布式训练框架了解多少？
64. 请比较决策树和随机森林的区别及优缺点
65. GBT是什么？LightGBM和XGBoost有什么区别？
66. 随机森林、LightGBM、XGBoost的集成方式有何不同？
67. 计算attention时为什么要除以√d_k？d_k的含义是什么？
68. 请介绍几个强化学习的常用方法
69. 多轮对话如何保留历史记忆？
70. 算子融合为什么能加速计算？
71. 强化学习与SFT有什么区别？
72. FM(因子分解机)和FFM(场感知因子分解机)是什么？
73. 请解释flash attention的原理
74. 请讲解CLIP的数据样例是什么样的
75. 本科期间参与的机器翻译项目具体情况？
76. 实习期间主要工作内容是什么？遇到的最大难题如何解决？
77. 数据集是如何设置的？测试了哪些大模型？
78. 大模型是私有化部署还是调用API？为什么选择私有化部署？
79. 请说明BERT模型结构的后续改进工作
80. 编写代码：求解最长递增子序列
81. 数据集构建的细节是什么？
82. 如何设计聊天场景中的角色扮演功能？
83. 用实际案例讲解Transformer的输入输出过程
84. 请介绍TF-IDF算法
85. 提高模型问答效果需要从哪些方面入手？
86. BERT和GPT的主要区别是什么？
87. 如何保障模型输出遵循设定规则？
88. RAG项目中知识库是如何构建的？使用哪个模型进行嵌入？
89. 项目中遇到过拟合现象吗？如何解决的？
90. 请实现矩阵乘法的C++三重循环版本
91. 如何控制大模型输出的稳定性？
92. 在对话场景中如何更好地识别用户意图？
93. 请介绍RoPE(旋转位置编码)
94. 请介绍Transformer架构
95. 目前有哪些游戏接入了大模型？
96. 数据集应该如何构建？如何评估数据质量？
97. 如何设计可随意切换个性的问答系统？
98. self-attention公式为什么要除以√d_k？cross-attention呢？
99. 请解释softmax函数在attention中的作用
100. 编写代码：计算岛屿数量(ACM模式)
101. 请描述MHA的实现步骤
102. LLaMA和Transformer有什么区别？
103. 请介绍DeepSpeed框架
```

# 面试经验

```markdown
1. 面试中项目话术未用上，感觉表现不理想
2. 面试官重点关注大模型下游任务，如游戏NPC相关的角色扮演和对话NLP任务拆解
3. 岗位更偏好有传统NLP经验和大模型使用经验，以及业务算法落地经验的同学
4. 我的实习经历与岗位要求的RAG优化和模型能力提升匹配度不高
5. 询问了推理优化的具体内容，面试官回答较模糊，表示是基于自建模型的定制优化
6. 确认了使用的是开源模型微调而非自行训练
7. 推理引擎使用Triton Server，主要工作是参数调优而非底层算子开发
8. 确认了部门主要做智能NPC业务，我对此表现出强烈兴趣
9. 整体面试没有技术问题和算法题，可能是主管面试
10. 面试时长26分钟，面试官像是主管
11. 展示论文项目时通过共享屏幕讲解，但对方几乎没有提问
12. 面试官比较严肃，交流意愿不强
13. 有两个面试官，态度都很和蔼
14. 二面时面试官原本要求写MHA后改为MLA，被同事指出错误
15. 一周后HR回复仍在筛选，建议保持耐心但最终未通过
16. 自我评估基础不够扎实，缺乏分布式训练实践经验
17. 整体感觉公司招聘需求不紧急，面试不太顺利
18. 近期面试较少导致有些生疏
19. 面试官专业耐心，但自觉水平有限
20. 面试时有些紧张，算法题出现小失误
21. 请分享编写提示词(prompt)的经验
22. 如何判断提示词是否合格及改进方法
23. 设计角色设定有哪些技巧
```
# 字节跳动

创建时间: 2025年5月14日 00:04

# 面试题目

```markdown
1. 请详细说明如何实现Agent架构？  
2. 能否分享一下写prompt的心得？  
3. 在你的项目中使用了多少个Agent？  
4. code_interpreter部分是否有QA调试？  
5. 多个Agent之间是否进行了对齐？  
6. 请介绍大模型LoRA技术的原理和架构。  
7. 请继续讨论LoRA的训练技术。  
8. 熟悉Llama模型吗？能否说明它的三点变化？  
9. 请解释LoRA的整体架构及其优势。  
10. 为什么LoRA能降低显存使用？  
11. GPT基于Transformer的哪些部分？  
12. 你了解国内大模型的发展情况吗？  
13. 请手写MHA（Multi-Head Attention）和DPO（Direct Preference Optimization）算法。  
14. MQA（Multi-Query Attention）相比Multi-Head Attention有哪些优化？  
15. 请介绍你与岗位相关的Agent实习项目。  
16. 请解释价值模型的原理。  
17. GAN与Diffusion模型的主要区别是什么？  
18. 为什么现在许多模型改用MQA（Multi-Query Attention）？优化了哪些方面？  
19. 请讲解DeepSpeed的原理及其作用。  
20. 请讲解PPO（Proximal Policy Optimization）算法，并推导公式。  
21. 有哪些Transformer加速方法？  
22. MLA（Multi-Head Latent Attention）如何与RoPE（Rotary Position Embedding）融合？  
23. ds-r1的MoE（Mixture of Experts）与普通MoE有何区别？  
24. 你了解多模态模型吗？  
25. 请讲解DPO（Direct Preference Optimization）公式。  
26. LoRA的原理是什么？  
27. 请详细介绍CLIP和BLIP模型。  
28. 请推导DDPM（Denoising Diffusion Probabilistic Models）公式。  
29. 请阐述Transformer的结构。  
30. LoRA在训练和推理过程中是否有额外计算？  
31. 注意力机制的维度dk是什么？  
32. self-attention和cross-attention的区别是什么？  
33. 你使用过Qwen的图文生成和文字生成图像模型吗？  
34. 如何评价Stable Diffusion的生成结果？使用哪些定量指标？  
35. 为什么GAN训练不稳定？  
36. 你在微调大模型时遇到过哪些难题？如何解决的？  
37. 你对国产大模型的理解是什么？它们存在哪些问题和瓶颈？  
38. 多个Agent是否专门做过对齐？  
39. 大模型如何进行微调？  
40. 你了解LoRA吗？它的两种矩阵初始化方式是什么？如何用PyTorch实现？能否交换初始化方式？  
41. 如果给你500条数据集，每条数据存在相似或矛盾语义关系，如何判断？如何处理数据？500条数据是否足够微调大模型？  
42. 请手写代码求解两个有序数组的中位数，时间复杂度要求O(log(m+n))。  
43. 请具体讲解模型轻量化方法中的量化技术。  
44. 在工程中如何优化ComfyUI的推理性能？  
45. 如何实现合并k个已排序的链表？  
46. 如何实现Agent架构？  
47. Stable Diffusion有哪些知名插件？  
48. C++的五种内存类型包括哪些？  
49. 你了解哪些分布式训练方法？  
50. 请介绍ControlNet的原理。  
51. 自回归图像生成是如何实现的？  
52. Pixel Diffusion有哪些应用？  
53. 你了解MoE（Mixture of Experts）吗？通常如何设置专家数量？如何均衡负载？门控的训练方法是什么？  
54. 为什么要使用价值模型？  
55. 在不同任务中如何选择损失函数？谈谈你对各种损失函数的理解。  
56. 请讲解Adapter相关方法，包括T2I-Adapter和IP-Adapter。  
57. 请讲解Transformer的结构。  
58. Diffusion模型的采样方式有哪些？DPM+和DPM++是什么？  
59. 面向对象编程的三个要素是什么？  
60. 请手写MHA和DPO算法。  
61. 你了解Llama、GPT-2等国内大模型的结构吗？它们的共同点和不同点是什么？  
62. 如何提升模型训练速度，使其更快收敛？  
63. DDPM和DDIM的关系是什么？  
64. 你了解Inpainting技术吗？  
65. 在项目中，检测优化的技巧有哪些？重参数化如何实现？  
66. 如果你与一个人比赛但胜率较低，会选择五局三胜还是三局两胜？为什么？  
67. 请解释Softmax的底层原理。  
68. 你有API开发经验吗？  
69. 这些模型的异同点是什么？你认为哪种方法更好？  
70. DPO的奖励分数如何计算？  
71. 请解释Textual Inversion和LoRA的作用。  
72. 请解释LayerNorm和BatchNorm的底层原理。  
73. 请手写KL散度公式。  
74. 项目中的特征提取方法是什么？如何优化模型？  
75. 请讲述Imagen模型。  
76. CLIP有哪些变体？  
77. 什么是野指针？（注意：不是内存泄漏）  
78. Python的底层通信机制是什么？  
79. 请介绍LoRA的参数r和alpha的作用。  
80. 请详细描述各种并行算法的过程，并量化计算量和通信量。  
81. 请讲解LoRA的实现细节和PPO的实现细节。  
82. 你了解大语言模型常用的旋转位置编码（RoPE）吗？  
83. 你了解DeepSeek的GRPO吗？  
84. 你对多模态模型了解多少？  
85. 请解释深拷贝和浅拷贝的区别。  
86. 请讲解模型优化的技巧、损失函数以及生成对抗网络。  
87. PyTorch中`nn.eval()`与训练模式的区别是什么？BN和Dropout在训练和测试时的区别是什么？  
88. 请设计一道中等难度的算法题并讲解。  
89. 请讲解Stable Diffusion的完整架构及其各部分的原理。  
90. 求整数列表的最大子序列和。  
91. 请详细讲解DALL·E 1到DALL·E 3的发展过程，每个版本的核心算法及创新点。  
92. 请介绍PPO（Proximal Policy Optimization）。  
93. 请介绍CLIP的原理及其训练方法。  
94. 开放题：如何生成山寨Logo？请给出具体方案。  
95. 请讲解Python中的装饰器（Decorator）。  
96. 如何加速Diffusion模型的生成过程？  
97. 手撕无重复字符的最长子串，提供思路和代码（限时5分钟）。  
98. T5和LLaMA模型的异同点是什么？  
99. 请讲述你对Stable Diffusion中蒸馏方法的了解。  
100. 这个项目中使用了多少个Agent？  
101. 什么是智能指针？它是如何起作用的？  
102. 项目中LoRA的两个矩阵是如何初始化的？  
103. 请实现倒序的长链表分段。  
104. 预归一化和后归一化的区别是什么？  
105. MHA（Multi-Head Attention）的概念是什么？  
106. RL（强化学习）中基于策略的采样问题是什么？  
107. 如何设计Reward函数？Actor-Critic的调参方法有哪些？  
108. 请讲解Stable Diffusion的训练和推理过程。  
109. r1训练步骤是什么？GRPO的改进有哪些？GRPO是过程奖励还是结果奖励？MLA的做法及其优势是什么？  
110. 请实现大数相加功能。  
111. 手撕MHA（Multi-Head Attention）代码。  
112. 求数组中第k大的数（限时秒杀）。  
113. Python中的可变类型和不可变类型有哪些？  
114. 手撕一道双指针字符串题。
```

# 面试经验

```markdown
1. 二面大约20分钟，面试官在公司外面，显得很赶时间  
2. 面试官让我做自我介绍并介绍科研项目  
3. 介绍科研经历时被中途打断  
4. 被问了一些与科研相关的基础问题，但没有深入探讨  
5. 询问后续流程，面试官说二面是终面，但可能会根据情况加面  
6. 二面没有要求手撕代码或解答算法题  
7. 感觉通过机会不大，但会继续关注后续结果  
8. 面试内容主要围绕项目和岗位相关知识展开  
9. 我的背景比较特殊，之前实习涉及后端和算法，实践经验较多  
10. 感觉字节更看重实际能力，对学历要求不像华为那么严格  
11. 从一面到三面，感觉面试压力逐渐降低  
12. 三面的技术leader主要关注项目细节和落地情况  
13. 三面没有深入讨论技术原理，主要询问项目实现情况  
14. 三面没有代码题，持续了45分钟左右的交谈  
15. 整体面试难度呈现先升后降的趋势  
16. 大厂面试问题难度较大，切入点很直接  
17. 三面难度较低，类似二面的简化版，还讨论了实习规划  
18. 一面面试官在国外，沟通不够详细，时间紧张  
19. 一面询问了科研经历，包括数据准备、预处理方法和使用的大模型  
20. 一面有常规的技术问题提问  
21. 一面的代码题是实现sqrt函数，其他是简单leetcode题  
22. 询问一面后续流程，被告知可能还需3-4轮面试  
23. 两小时后收到HR通知一面通过，并预约了二面时间  
24. 本以为加面会偏业务讨论，结果是一面的升级版  
25. 加面问题突然变得很细，有些措手不及  
26. 感觉加面像是KPI考核，全程未开摄像头  
27. 加面结果表明还需要继续努力  
28. 二面后等待多天才约HR面，可能是在对比候选人  
29. 等待期间心理压力较大  
30. 因为实习经历相关，被询问了约20分钟  
31. 一面和二面之间间隔1天  
32. 讨论实习相关内容持续了30分钟  
33. 最终顺利拿到offer，认为内推和针对性准备很关键  
34. 面试前参考了牛客网字节算法岗面经  
35. 针对性地加强了八股文和代码题的准备  
36. 字节面试侧重前沿技术理论考察  
37. 个人项目经历被问得较少（可能因为经验不足）  
38. 面试前在别人的面经里看到类似题目很庆幸  
39. 建议想去大厂的同学多参考前辈的面试经验  
40. 提前熟悉面试流程和技巧很重要
```
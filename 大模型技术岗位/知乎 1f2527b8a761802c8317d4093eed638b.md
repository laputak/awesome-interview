# 知乎

创建时间: 2025年5月14日 00:22

# 面试题目

```markdown
1. BERT、GPT等主流大模型在细节上有哪些不同？比如位置编码、训练损失、激活函数和架构等方面。自回归模型有什么特点？
2. 请讲解PPO与GRPO的区别
3. 从数据角度看，大模型训练存在哪些问题？
4. MOE架构是什么？有哪些特点？
5. DPO的奖励是基于token粒度还是sentence粒度？
6. 针对模型安全，有哪些防护措施可以实施？有量化指标吗？
7. 分布式训练有哪些常用技术？
8. 模型安全方面有哪些做法？有哪些常见的注入攻击手段？
9. LLAMA的输入长度可以无限长吗？输入变长会带来哪些变化？
10. 大模型常用哪些激活函数？GELU有什么特点？为什么有效？
11. 多模态大模型通常采用什么结构？
12. 请介绍PPO算法
13. 如何训练Function call功能？如何进行微调？
14. llama为什么使用RMSNorm？RMSNorm为什么有效？PRMSNorm又是什么？
15. LLM出现复读机现象的原因是什么？如何解决？
16. GLM是prefix LM还是casual model？是decoder only模型吗？两者有什么区别？
17. 百川、千问、LLAMA的Position Embedding是如何设计的？有什么区别？
18. 你知道哪些注意力机制？
19. Transformer的基本结构是什么？
20. 模型涌现现象的原因是什么？
21. 对于特定任务，应该使用预训练还是微调？
22. 如何理解大模型安全及其包含的内容？
23. Encoder-Decoder、Casual Decoder和Prefix Decoder有什么区别？
24. 请解释各种范数的概念，并手写公式说明应用原理
25. 算法题：求解三数之和问题
26. 多头注意力的时间和空间复杂度是多少？有哪些优化方法(kv缓存、多查询注意力、组查询注意力)？请手写多头注意力代码
27. 你微调过哪些模型？微调占用的显存大小是多少？与哪些因素有关？
28. llama是如何优化注意力机制计算的？
29. RAG和微调的区别及各自的优缺点是什么？
30. 如何组织Function call的文本格式供模型使用？
31. 请讲解R1和GRPO算法
32. 你了解多模态技术吗？
33. 请介绍Qwen模型的结构
34. 大模型训练有哪些主要步骤？
35. 请手写自注意力机制的公式
36. RAG的具体过程是什么？
37. LoRA的初始化方式是什么？为什么降维矩阵用高斯初始化而升维矩阵用零初始化？如果反过来会怎样？都用零初始化或都用高斯初始化会怎样？
38. 算法题：删除链表倒数第n个节点
39. 请讲一下GROUP算法
40. RLHF的具体过程涉及几个模型？
41. 不同模型结构各有什么优势？
42. SFT导致的通用能力遗忘如何解决？
43. 如何将下游工具和插件转换为模型可理解的格式？
44. 手撕代码：实现二叉树的层序遍历
45. 面试主要询问了哪些实习经历？
46. 在介绍岗位时，即使没接触过多模态，也要能根据招聘信息补充相关知识，至少说明基本概念
47. 简历上的技术栈必须非常熟悉，能应对面试官的深入追问
48. 需要掌握最新热门技术，比如近期deepseek相关的技术
49. 请介绍你参与过的与大模型相关的项目
50. 在实习过程中有哪些创新点？效果如何？
51. 请讲述你了解的大模型技术最新发展
52. 哪些大模型产品给你留下了深刻印象？
53. 你们是如何跟进最新的大模型技术的？
```

# 面试经验

```markdown
1. 面试官详细询问了实习经历的具体细节
2. 大模型方向竞争很激烈，面试遇到很多新模型和新论文，更新速度比阅读速度还快
3. 年后开始认真准备面试，参考居丽叶的项目梳理方法很有收获，之后两周高强度背诵面试模板、刷LeetCode和整理项目
4. 大模型面试本身不难，但需要充分准备
5. 面试已经持续了一个小时
6. 面试官详细询问了实习经历
7. 有转正机会但需要申请和产出，等待后续通知
8. 目前大多数公司集中在语言模型研究，多模态预训练是研究方向之一
9. 两轮面试表现一般，因为开题答辩耽误了几天。感觉比之前面大厂时表现好，但不知道为什么被拒
10. 建议多做编程练习题
11. 建议多参加面试积累经验
12. 编程题很重要，其他问题没答好但代码过了还有机会，代码不过就很难通过。这需要长期练习
13. 建议重点准备AI大模型的常见面试题，特别是大厂通用题目，再针对性地整理项目相关问题
```
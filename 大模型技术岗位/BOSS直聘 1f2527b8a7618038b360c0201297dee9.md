# BOSS直聘

创建时间: 2025年5月14日 00:18

# 面试题目

```markdown
1. 如果你需要构建一个实时数据处理系统，你会优先考虑哪些技术和架构？
2. 请描述一次你在项目中遇到失败的经历，以及你从中学习到了什么。
3. 面试中没有让手撕代码算法题。
4. 请描述你参与的一次大型算法项目，包括你的角色和具体贡献。
5. 面试中询问了所有项目，没有区别对待。
6. 假设你负责优化一个推荐系统，但用户反馈不佳，你会采取什么步骤？
7. Boss直聘多模态算法工程师（预训练）一面挂。
8. 请分享一次你在团队中遇到重大技术分歧的经历，以及你是如何解决的。
9. 突然生硬地问到Transformer的固定问题。
10. 下午面试一家中型工厂，问题偏工程技术，而我侧重开源项目和算法理论研究，难以应对。
11. 你上一份工作中成功推动了哪些算法改进，能否具体描述一下？
12. 请介绍Multi-Query Attention。
13. 大模型（LLMs）有哪些缺点？
14. 介绍使用Swish的GLU块的计算公式。
15. 数学的本质是什么？
16. prefix Decoder、causal Decoder和Encoder-Decoder的区别是什么？
17. 在保护用户隐私的同时，联邦学习允许多个设备或服务器协同训练模型。
18. 请阐述Transformer结构。
19. 您使用过Qwen的图文生成和文图生成模型吗？
20. 点积运算建立Token间的语义相似度度量空间。
21. 大模型LLM的训练目标是什么？
22. 你在大模型微调中遇到过什么难题，是如何解决的？
23. 请介绍大模型（LLMs）。
24. 对现有国产大模型的理解及其存在的问题和瓶颈是什么？
25. Transformer模型自注意力机制的引入使模型更好地理解文本上下文关系。
26. 大模型（LLMs）有哪些优点？
27. 强化学习通过与环境交互学习最优策略，在游戏和机器人控制等领域展现出巨大潜力。
28. 请解释LSTM和GRU的工作原理及应用场景差异。
29. Layer Norm的计算公式是什么？
30. 在算法开发过程中，你常用哪些工具或框架，原因是什么？
31. 将归一化后的权重与对应的Token值向量相乘以进行信息过滤。
32. Attention的变体有哪些？
33. Query向量表示当前Token的注意力焦点，类似于提问：其他Token中哪些信息与我相关。
34. 请写一道简单的贝叶斯概率题。
35. a2的Query向量与所有Token的Key向量进行点积运算，生成4个原始注意力分数α。
36. LN在LLMs中的不同位置有何区别？如有，能否介绍这些区别？
37. 请写一下Deep Norm的代码实现。
38. 对加权后的Value向量求和，生成最终特征融合结果b2。
39. 该过程实现了动态特征路由机制。
40. 为什么现在的大模型大部分采用Decoder-only结构？
41. 传统Attention存在哪些问题？
42. 模型蒸馏、RAG和微调的区别是什么？
43. 是否有API开发经验？
44. Multi-head Attention存在什么问题？
45. 相关性建模是什么？
46. 多模态学习结合文本、图像、音频等多种数据类型，提供更丰富的用户体验。
47. 大模型（LLMs）后面的175B、60B、540B等指什么？
48. LLMs各模型分别使用了哪种层归一化？
49. 结合大模型和知识图谱，可以构建更智能的问答系统和推荐系统。
50. 项目二深入探讨：BERT模型的微调方法，如何实现并行处理，双编码器的概念，词粒度编码器的原理及应用。
51. 特征向量化（Q、K、V投影）是什么？
52. Deep Norm的思路是什么？
53. 手撕交叉熵函数，包括函数变体、变量更换及其对函数的影响。
54. Deep Norm有哪些优点？
55. 如何实现数组的扁平化？
56. transformer八股包括全流程。mha为何多头，为何除以dk，为何有qk，直接kk点乘会怎样。
57. 项目一的深入探讨：如何改进模型，如何处理数据集，介绍框架等细节。
58. 介绍使用GLU线性门控单元的FFN块计算公式。
59. 对原始分数进行Softmax归一化，转化为概率分布图示Attention Weights。
60. BERT（Bidirectional Encoder Representations from Transformers）通过双向编码器，能更准确捕捉语言细微差别。
61. Key向量（特征索引器）：存储可检索的语义特征，相当于建立索引目录：我的关键特征以这种方式编码。
62. RMS Norm相比于Layer Norm有什么特点？
63. YOLO（You Only Look Once）是一种快速的目标检测算法，适用于实时视频分析。
64. 涌现能力的原因是什么？
65. 写代码，未见过但不难，查找数组中重复两次的数。
66. 目前主流的开源模型体系有哪些？
67. 通过利用数据的结构信息，自监督学习减少了对大量标注数据的依赖。
68. 加权求和：执行基于相关性的特征重组。
69. 面试内容为：Python基础知识，如dict是否有序，is和==的作用等。
70. T5和LLaMA模型有何异同点？
71. softmax：构建特征选择的概率分布。
72. FFN块的计算公式是什么？
73. 项目中LoRA的原理，两个矩阵是如何初始化的？
74. 请谈谈你对KNN、SVM和决策树算法优缺点的分析。
75. 元学习使模型能快速适应新任务通过学习如何学习。
76. 通过生成器和判别器的对抗训练，GAN能够生成逼真的图像、音频或文本。
77. 大模型项目深入探讨，基本了解流程后结束，内容简短。
78. 价值向量（语义编码器）：承载细粒度语义信息，如同信息仓库我的深层语义特征在此呈现。
79. 每个输入Token通过三个独立的线性变换生成。
80. 介绍Swish计算公式。
81. 介绍一下超参数调优的方法，你如何进行模型调优。
82. 卷积神经网络（CNN）在图像识别和分类中表现出色，能够自动提取图像特征。
83. RMS Norm的计算公式是什么？
84. 比较Multi-head Attention和Multi-Query Attention的区别。
85. 介绍一下GeLU的计算公式。
86. 结合神经网络的学习能力和符号逻辑的推理能力，这种集成方法有望解决更复杂任务。
```

# 面试经验

```markdown
1. 面试以聊天为主，技术问题较少
2. HR耐心询问心仪岗位，电话沟通基本情况和现有offer
3. 询问后续流程，说明第二轮是终面，可能根据情况增加环节
4. 一面结束两小时后收到通过通知并预约二面时间
5. 面试官在公司外面试，显得时间紧张
6. 科研经历介绍被打断，只问了常规科研问题未深入探讨
7. Boss直聘多模态算法工程师(预训练)一面未通过
8. 面试官提问气势较强，主要考察模型理解，部分问题思考较久，感到压力
9. 已准备签三方协议，HR建议尽快开始并承诺一周内给结果
10. 面试官在国外导致交流不够详细，时间仓促
11. Boss直聘职位JD描述笼统，面试官解释实际做搜索LLM的A/B测试
12. 1月初投递字节大模型日常实习岗，刚完成二面感觉不理想
13. 参加初创公司大模型面试表现尚可，最终未获offer可能因技术栈不匹配或年级问题
14. 中年面试官态度温和，过程轻松，主要关注项目细节
15. 实习准备中最难的是方向调整和多次未通过后的心态管理
16. 笔试到offer一周内完成，流程顺利
17. 请介绍你的三个核心优势
18. 如果有两个月带薪假期最想做什么
19. 如何快速抓住岗位JD关键词进行精准匹配
20. 如何在压力面试中从容展现逻辑思维能力
```
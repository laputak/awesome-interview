# 滴滴

创建时间: 2025年5月14日 00:18

# 面试题目

```markdown
1. 在编程过程中调试了两次，感觉基础知识不够扎实。  
2. 滴滴NLP的算法面试比较基础。  
3. 对理论不熟悉，应用理论的能力不足。  
4. 八皇后问题不会写。  
5. torch.nn 和 torch.nn.functional 的异同是什么？  
6. 对运筹学优化问题的理解。  
7. AUC指标的物理含义是什么？  
8. 深入探讨Transformer的底层原理，包括输入维度、过程中的维度变化及各种细节。  
9. L1和L2正则化的区别是什么？  
10. 激活函数的作用是什么？  
11. 二面，求二叉搜索树第k小的值。  
12. 了解哪些大模型？ChatGPT的训练过程是怎样的？  
13. Actor-Critic原理是什么？  
14. 训练一个reward模型（标注最佳项而非成对比较）的损失函数如何编写？  
15. 智力题：ABCD四人，两黑两白帽子，B能看到C，A能看到BC。四分钟后，一人知道自己的帽子颜色，是谁？较简单。  
16. 准备好草稿纸和笔，准备解答简单的条件概率问题。  
17. 7B模型的参数量如何估算，分布在哪些部分？  
18. 树模型和XGB模型的特征重要性如何计算？  
19. 什么是reward hacking？如何处理？  
20. DIN模型结构，稀疏特征等。数值特征只归一化吗，还了解其他处理方式吗？了解DIN模型的进一步拓展吗？激活单元的作用和结构。  
21. ReLU和Dice激活函数的区别是什么？了解其他激活函数吗？知道ReLU的变种吗？  
22. itemCF如何计算相似度，是否了解其他方法？  
23. 手撕LeetCode，掌握回溯算法，熟悉组合与排列问题。  
24. 协同过滤召回如何实现？  
25. DPO的学习率如何选择？  
26. 字母异位词（滑动窗口）相关问题。  
27. 请分别介绍一下deepspeed的原理，特别是Zero 1、2、3。  
28. DPO训练时关注哪些指标？  
29. DEEPer1有哪些技术给你留下了深刻印象？  
30. llama的结构相比transformer有哪些优化，主要介绍了rope、rmsnorm、swiglu等。  
31. 三面，判断一棵树是否为二叉搜索树。  
32. 删除链表中倒数第n个节点。  
33. 了解FM原理，能讲解DeepFM原理吗？  
34. 介绍LR模型，损失函数是衡量预测值与真实值差异的指标。介绍交叉熵损失函数，用于衡量概率分布之间的差异。  
35. 手撕LeetCode，二叉树，最近公共祖先。面试官改为快速排序。  
36. 评估指标：AUC、召回率、精确率。  
37. 手撕LeetCode，滑动窗口，最长不重复子串问题。  
38. 使用梯度更新神经网络的原理是什么？  
39. 生成有效括号组合。  
40. 请介绍实习期间的视频分类项目，使用了哪种模型，具体结构是什么，用了Conformer模型。  
41. XGBoost的并行计算能力。  
42. value-based和policy-based的区别是什么？  
43. 面试内容为：本地IDE手撕self-attention前向过程，简要写下类初始化和self-attention公式。  
44. 你了解哪些大模型？  
45. Deep and Wide模型。  
46. 手撕：1. 八皇后 2. 在排序数组中找到所有等于 target 的两个数的下标。  
47. 面试内容为：使用归并排序算法排序链表。  
48. 强化学习的损失函数是什么？  
49. 过拟合的处理方法有哪些？Batch Normalization如何减少过拟合？  
50. Qwen模型原理，Transformer参数量主要在哪几部分，哪一部分参数量占比最大。  
51. 之前实习做过早期的NLP项目，如智能语音会议室预约系统，采用级联式设计。尽管当前大模型倾向于端到端方法，但早期思路仍有参考价值。最近也在阅读语音大模型相关论文。  
52. 算法题：在二维矩阵中搜索目标值，要求时间复杂度低。  
53. 什么是梯度？如何进行梯度下降？介绍其他梯度下降优化算法。  
54. 7B和70B模型的学习率有何不同？  
55. 如何解决用户冷启动问题？  
56. 了解哪些高效微调方法？简单介绍了qlora和ptuning。  
57. 二叉树的层序遍历是什么？  
58. 如何编写训练reward模型（pair对）的损失函数？  
59. 了解GRPO吗？与PPO的区别是什么？  
60. 投点法，一根一米长的绳子分成三段，形成三角形的概率是多少？  
61. Lora原理，微调时新增参数量多少？  
62. 面试内容为：询问关于BPE原理的分词相关知识。  
63. 对比XGBoost、LightGBM和CatBoost，XGBoost如何处理缺失值？了解CatBoost的特征并行处理吗？还问了一些问题，已忘记。  
64. 除了知识图谱，你还了解哪些机器学习和深度学习模型，请分别列举并介绍。  
65. 请描述如何使用广度优先搜索算法解决二叉树相关问题。  
66. 面试中被三次问到LoRA原理，包括整体原理、两个矩阵的初始化方法以及矩阵秩的选取。  
67. 项目中负采样是如何进行的，word2vec如何得到词嵌入？  
68. 面试问题包括召回方式、重排方式、embedding模型选取、召回不准的解决方法、幻觉问题的解决、后续优化方向等。  
69. 最近看了哪些前沿论文，请介绍一下。  
70. 介绍BERT模型及其预训练任务。BERT模型由编码器层组成。为什么BERT是双向的。  
71. MC DP与TD的区别？  
72. 贝尔曼方程的原理是什么？  
73. 一面，求能盛最多水的量，为LeetCode热门题目前100题目。  
74. 问我目前大模型可优化的两点：长上下文和幻觉相关问题。  
75. 面试内容为：必须深入理解模型的底层细节。  
76. 您觉得我哪方面还需要提升？您的项目经历很丰富，但细节部分可以再多写一些。  
77. 实习主要负责哪些业务（ASR，TTS）。
```

# 面试经验

```markdown
1. 滴滴采用车轮战面试形式，一下午连续面试三次，每次间隔十几二十分钟。最后人都麻木了。本想每次面试后记录，但结束后完全记不清，只能回忆大致问题。每场面试约50分钟。
2. 面试官话不多，没有给太多压力，但反应比较平淡，可能结果不太理想。
3. 滴滴面试流程很快：8月12日一面，8月14日二面，二面半小时后就通知通过，至少有班上了。
4. 上次滴滴二面全是八股题，这次准备了八股题结果全是手撕题。面试官很好，见我第二个没撕出来还换了题目。
5. 4月3日原定5场面试，其中一场因面试官临时有事取消，实际和昨天一样只面了5场。
6. 滴滴面试体验很好，结构清晰：20-30分钟提问，20分钟手写代码，10分钟提问环节。
7. 一面问题很多，感觉时长是滴滴算法另一部门一面的两倍。手写代码稍有延迟，提问非常详细，直到问到我不熟悉的内容才结束。
8. 一面线上进行，整体氛围比较轻松。刚面完不久就收到了二面消息。
9. 二面在线下，整体偏压力面试。
10. 面试官比较关心学习能力。
11. HR次日来电约第四轮面试。如果第四轮失败，之前的投入就白费了。看来得抓紧刷力扣。
12. 感觉第三轮面试最好，可能因为面试官是主管，确实很有亲和力。第三轮面试后通常多久出结果？
13. 面试官提前进入会议，发现我已经在内，于是我们15:40就开始了。
14. 一天三面，论文问题问得很深，但常规问题较少。
15. 使用赛码网手撕代码，不运行，面试官手动检查，重视思路。
```
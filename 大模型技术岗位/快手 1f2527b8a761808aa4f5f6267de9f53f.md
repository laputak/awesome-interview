# 快手

创建时间: 2025年5月14日 00:17

# 面试题目

```markdown
1. 实习中遇到的困难是什么？
2. 在科研中遇到的困难及克服方法是什么？
3. 你在上一段实习中最困难的工作是什么，或者你收获了什么？
4. 为什么想转搜广推职位？
5. 你的研究方向是什么？
6. MMOE项目是如何来的？
7. 模型评估是如何进行的？
8. 请详细说明计算损失的步骤？
9. DPO原理是什么？
10. 大模型在短视频领域的应用有哪些想法？
11. 请介绍Listwise损失函数？
12. 是否有构建prompt的心得和技巧？
13. 如何估计模型训练的显存占用？
14. Bert有哪些模型变种如Roberta等？
15. 您是否了解生成式推荐？
16. 强化学习在搜索场景中的应用是什么？
17. 实习项目中的数据是如何处理的？
18. 幻觉问题的缓解方法有哪些？
19. 超参数有哪些？如何选择学习率？如何寻找超参数？小规模数据如何运行，是串行还是并行？如何查看loss曲线？
20. 在模型预测时，softmax是否将词表中所有token的概率归一化？词表通常很大，如何加速计算？
21. 如何缓解OOM问题？
22. 您是否有尝试过强化学习？
23. 如何解决模型幻觉？分方法、数据、prompt、微调、RAG等。
24. long-context通常有哪些问题，如何解决？
25. 项目设计框架是什么？
26. 请解释attention机制？
27. 位置内插和外推是什么意思？
28. DPO和PPO的区别是什么？PPO需要训练几个模型？如果固定Critic模型会有什么后果，为什么要更新？
29. 偏好数据集是什么？偏好如何定义？
30. 多模态大模型中text和image有哪些融合方式？
31. RoBERTa做出了哪些改进？
32. 知识图谱和大模型的关系是什么？各自能为对方提供哪些支持点？
33. 您了解链路一致性吗？
34. 模型训练与优化（超参数设置、评估方法、并行策略）？
35. VLM中vision encoder的计算效率测试过吗？
36. 多路召回是如何实现的？
37. 为什么使用word2vec生成词嵌入，是否与嵌入层生成的进行了对比？
38. 什么是BN？与LN的差异是什么？
39. 如何进行SFT？
40. 门控机制是为了解决什么问题提出的？
41. 您是否了解图像生成模型？
42. 什么是目标注意力机制？
43. 注意力机制的计算公式是什么？
44. BM25算法原理是什么？稀疏召回和稠密召回的优缺点是什么？RAG过程中如何提升指标？
45. 对比学习损失如何构造？
46. MMOE的MM代表Multigate Mixture，了解单个gate的情况吗？
47. 闪存注意力机制是什么？
48. 如何改进当前的专家网络？
49. 请手写多头注意力机制代码？
50. 网络中哪部分使query的最终向量长度一致？
51. 自注意力公式是什么？
52. 如何构建数据集？
53. 基础模型预训练是什么？
54. 请介绍Wide&Deep和DCN这两个模型？
55. MMOE与PLE的区别是什么？
56. 多模态大模型如何使大语言模型连接视觉模态？
57. Transformer-based模型主要有哪些？
58. 为什么要进行标准化？
59. 指标对比基线提升较多，你认为主要原因是什么？
60. PLE架构是什么？
61. 了解MOE吗？
62. 召回与粗排的区别是什么？
63. 注意力机制其他应用有哪些？
64. DeepSpeed介绍，使用了zero_2配置？
65. Transformer和LLaMA的区别是什么？
66. BLEU-4的计算公式是什么？
67. 罗伯塔和德伯塔的改进是什么？
68. CoT如何设计？
69. 自注意力机制与交叉注意力机制的结构区别是什么？
70. 多模态大模型如何训练？
71. 你认为你的工作在工业界有哪些可能的应用？
72. SFT和RL分别适用于什么样的业务场景？
73. MEGTRON LM介绍，DeepSeep Zero2流程？
74. Decoder-Only模型有哪些？
75. ChatGPT训练的三个阶段是什么？
76. 以TF或torch为例，如何冻结部分参数？
77. 请谈谈对deepspeed的理解？
78. KL散度和交叉熵的区别和联系是什么？
79. 你了解哪些BERT的改进版本？
80. 了解哪些微调方式，如LoRA, LongLoRA, QLoRA, 前缀调优？
81. 核心技术细节包括BPE和MHA的实现？
82. 是否有进行量化和加速工作？
83. 请手写Multi-Head Attention代码？
84. SFT之后为什么还要进行RL？
85. 归并排序是什么？
86. 使用了什么模型？
87. 在特征筛选方面做了哪些工作？
88. 请介绍rope？
89. LORA介绍，秩的数学意义，如何计算矩阵的秩？
90. 什么是CTR和CVR？
91. 了解LLM中的强化学习吗？
92. 冒泡排序是什么？
93. 你如何构建MMOE这个项目？
94. Transformer相比LSTM的优势是什么？
95. CLIP的结构是什么？
96. 了解PLE之后的多任务模型吗？
97. 你使用的MMOE的损失函数是什么？
98. DPO损失公式是什么？
99. DIN的QKV分别代表什么？
100. 在Pytorch中如何自定义反向传播函数？
101. 召回的链路是怎样的？
102. 如何构建负样本？
103. RoBERTa去掉了哪些embedding？
104. 请详细说明CrossNetwork的结构及其特点？
105. 什么是断判，如何进行断判？
106. 了解MoE和Group？
107. long-context与sparse attention的结合？
108. MOEE极化时DROPOUT加在何处？
109. 如何解决幻觉问题？
110. 多路召回使用的特征是什么？
111. 为什么通常使用bf16？
112. 什么是BERT？
113. 混合负采样策略是什么？
114. 数据集规模、筛选与配比？
115. 编码器与解码器注意力机制的区别是什么，为什么解码器要使用掩码？
116. 快速排序及手写LLaMA实现？
117. LORA训练为什么可以节省显存？
118. 交叉熵的计算公式？前向传播时选择交叉熵作为损失函数的原因？为什么不使用MSE？
119. 什么是GQA？
120. 如何判断模型训练时是否存在过拟合？如何避免过拟合？
121. Transformer中哪些部分借鉴了前人的工作？
122. 向量模型和重排模型如何微调？
123. 训练时会冻结部分参数吗？
124. 梯度爆炸和梯度消失由网络深度增加或权重初始化不当等原因产生？如何避免？
125. 你跑过哪些NLP模型？
126. VLM信息完备性和幻觉如何权衡？
127. 您了解BART模型吗？
128. 排序损失函数和召回损失函数？
129. 召回使用什么损失函数，推理时如何保证效果？
130. BGE模型如何训练？
131. encoder-only降秩问题？
132. 在数据收集过程中是否有进行人工处理？
133. 你了解哪些注意力机制？
134. Transformer与RNN、LSTM的不同之处在于什么？
135. Bert后面衔接一个LSTM如何对接？接LSTM有何效果？直接用Bert做分类不行吗？
136. 您是否了解DeepSeek相关技术？
137. leetcode最大雨水面积（中等难度）思路：双指针方法？
138. DPO、PPO、GRPO有什么区别？
139. 如何使用多模态大模型？
140. CLIP的损失函数？
141. 请介绍Transformer的主要结构和流程？
142. 介绍一下Bert的结构？Bert的预训练任务是什么？Bert的MASK策略是什么？为什么Bert的三种编码可以相加？
143. 可以讲解一下Transformer的架构吗？
144. VLM有什么缩放定律吗？
145. 蒸馏效果不佳的原因是什么，采取了哪些额外措施？
146. 训练多目标的经验和遇到的困难？
147. in-context learning的示例如何采样？随机还是检索？
148. DCN的结构是什么？
149. KV缓存的原理是什么？
150. BERT与当前生成式大模型的区别？
151. 注意力公式是什么？
152. 目前多模态大模型存在哪些问题值得解决？
153. 面试内容为：encoder-only、decoder-only和encoder-decoder模型？
154. 混合精度训练如何进行？
155. 使用双指针方法？
156. 使用辅助栈保存最小值？
157. 请解释如何实现快速排序算法？
```

# 面试经验

```markdown
1. 面试官很和蔼，先介绍了业务内容。
2. 二面结束后是否还有三面？面试官表示通常为两轮技术面试，特殊情况可能进行三面。
3. 面试官总体很友好，大部分时间都在听我讲述经历。
4. 面试没有涉及写代码环节，总时长30分钟，详细询问了实习经历并经常追问原因。
5. HR表示正在横向对比候选人，十多天后未有结果，最终被放入人才库。
6. 面试后约3个小时就安排了第二天的第三轮面试。
7. 这次面试的职位是leader岗，面试官很友好，一开始就介绍了业务情况。
8. 面试官给人感觉很谦逊，而且很有专业高度。
9. 第二天进行了第二轮面试。
10. HR在面试结束后立即约我进行电话面试沟通。
11. 19:10接到HR电话通知面试通过，给予口头录用通知，三个工作日内发放正式录用通知。
12. 询问了薪资待遇相关问题。
13. 面试官在介绍业务时可能已经对你产生了兴趣。
14. 4月18日进行HR面试，与第二轮面试间隔两周，主要讨论个人意向。询问HR结果何时出来，HR表示当晚会与领导商量。
15. 快手的三轮面试体验都很满意，可能因为方向比较契合。
16. 6月4日周二14:00进行二面（leader面）1小时，先聊了30分钟论文，没有问八股文，感觉比一面轻松，做了一道简单编程题，完成后与面试官聊了一会儿，当时感觉基本通过。
17. 5月31日周五进行一面，历时一小时。前40分钟讨论论文，主要提问论文细节。中间未涉及常规问题。整体氛围轻松。最后15分钟做简单编程题。面试官介绍团队工作内容，随后进入反问环节。
18. 6月3日周一接到HR电话通知一面通过并约二面时间。
19. 面试感觉较为轻松，但自我感觉回答不够满意，仍成功通过。
20. 17:00进行HR面试，简单交流个人情况。HR需撰写面试总结。通常HR面试不会淘汰候选人。
21. 面试时间为1小时，整体难度不大，需要补充遗漏的八股文知识。
22. 因个人经历与岗位不匹配，一面后未通过，这也是去年暑期实习面试中唯一未通过的案例。
23. 应该先介绍项目背景，再说明具体细节。
24. 准备了一个PPT，详细介绍了每篇论文内容，用时约25分钟。
```
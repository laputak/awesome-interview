# 百度

创建时间: 2025年5月14日 00:15

# 面试题目

```markdown
1. 面试开始时直接要求背诵，未询问项目经验
2. 百度业务涉及大模型智能对话场景，包括意图识别、RAG、NL2SQL、信息抽取和tool call等环节
3. 代码表现良好，未被要求手写代码
4. 询问实习期间如何协调学校工作
5. 面试聚焦在序列建模和多目标建模经验，包括模型迭代和特征优化
6. 基础理论问题回答完整，但算法题表现不佳
7. 一面讨论愉快，涉及encoder和知识图谱的开放性话题，流程快速（1小时后约二面）
8. 对比DIEN和SIM模型的优劣势，包括后续发展了解
9. 详细说明实习工作内容、原因及支持的业务项目
10. 经典天平称球问题：12个球中找出重量异常的一个
11. 讲解GBDT原理
12. 比较SFT和RLHF的区别
13. 训练中使用的数据增强方法
14. 标签召回的数据量规模
15. 多头注意力中为何除以√dk，该值是否需要变化
16. 重复问题：为何除以√dk
17. 大模型常见的位置编码方式
18. 比较encoder-only、decoder-only和encoder-decoder架构区别
19. Llama模型的特点
20. Python中可变参数的处理方法
21. 语义召回的实现方式
22. 大模型微调后的性能评估方法
23. 翻译任务常用架构
24. 大模型改善的环节及优化建议
25. 数据集的构成
26. LoRA微调数据集的特点
27. 论文细节询问（无重大问题）
28. 缓解极化现象的方法
29. 项目基础理论考查
30. LN层的作用
31. 过拟合现象的定义及成因（模型复杂度、数据量、特征选择等）
32. 简历中模型原理说明：DIN结构、MMoE和CGC结构
33. DPO与SFT的区别及DPO的必要性
34. 遇到的最大挑战
35. 实习中的最大收获
36. 手撕代码：不开库实现平方根、最长公共子序列
37. DPO的问题及改进方法
38. 自注意力与多头注意力的区别
39. 能否手写multihead-attention
40. 算法题：寻找数组中第一个缺失的正整数（O(N)时间，O(1)空间）
41. 文本处理方法
42. 知识图谱在大模型时代的发展前景
43. PPO训练所需模型数量及更新策略
44. 大模型的架构类型
45. 比较BERT和GPT的训练方式及预训练任务细节
46. LoRA微调的秩确定方法，权重训练方式（与原权重联合或单独）
47. 排序实现方法
48. MMOE项目来源
49. 百度搜索从点击到展示的完整流程
50. self-attention公式、参数量、多头机制优势及√d的作用
51. 任务共享专家的使用判断依据
52. 概率论期望计算题
53. 大模型高效参数微调方法
54. RAG项目介绍
55. 特征处理细节：序列特征构造、长度选择依据、对齐处理等
56. 手写MHA
57. 简单贝叶斯概率题
58. 算法：寻找和为k的子数组（前缀和法）
59. 重复问题：DPO与SFT训练差异及模型数量
60. 与工程团队的合作方式
61. 数据分布情况
62. 文档解析方法
63. 假设BERT与GPT参数量相同，任务性能比较
64. Pandas数据筛选题思路
65. 复杂模型的使用理由及推理效率分析
66. MMOE项目介绍
67. 数据正负样本比例关注情况
68. 从CV转向搜广推的原因
69. Actor与SFT的关系
70. 图搜技术细节
71. GPT1到ChatGPT的发展历程
72. Transformer架构原理
73. RLHF中PPO、DPO和GRPO的区别
74. 大模型微调方式及微调层确定方法
75. NLP和知识图谱中的传统召回方法
76. 通过图片搜索定位真实坐标的方法
77. 旋转数组中二分查找目标值
78. attention机制工作原理
79. RAG使用的嵌入模型
80. 重复问题：RLHF中PPO和DPO的区别
81. LoRA的rank选择8的原因及其他参数尝试
82. encoder和decoder中多头注意力机制的差异
83. 螺旋矩阵相关问题
84. 大模型选择问题（llama3对比llama2的改进及性能优势）
85. 算法题：三数之和（LeetCode 15题）
86. actor和reference是否为同一模型
87. 业务场景流量规模、关键指标、线上线下收益差异分析及AB测试周期
88. 算法题：盛最多水的容器
89. Transformer多头注意力机制
90. Flash Attention原理
91. 坏案例处理方案（rag/微调）
92. BERT微调方法、并行处理实现、双编码器原理及词粒度编码器应用
93. 测试集表现优于训练集的原因分析
94. AUC计算方法
95. 自注意力机制解释
96. 评估方法及性能提升幅度
97. 最长回文子串算法题
98. 多模态大模型的使用经验
99. Transformer结构讲解
100. 手撕交叉熵函数及变体实现
101. 扩散模型相关问题
102. CLIP模型结构
103. 特征重要性判断方法
104. loss计算详解（i2t和t2i）
105. 数据库知识及大数据技术掌握情况
106. 数组扁平化实现
107. 应用场景及业务支持情况
108. 大数据集处理方法及单数据处理耗时
109. 模型改进方案、数据集处理及框架介绍
110. 梯度消失和爆炸现象
111. LLM提示构建思考题
112. 嵌入操作实现方法
113. DPO、PPO、GRPO的技术原理区别及数据准备差异
114. 缩放点积注意力公式
115. 多任务建模的必要性
116. 搜索pipeline流程
117. Transformer组件详解
118. actor/critic/reference/reward/SFT的区别
119. DPO中reference model的作用
120. 项目灵感来源（是否合作课题）
121. 项目技术细节：原理、优化、指标、架构等
122. SAM模型原理
123. Beam search原理
124. 不均衡文本分类优化方法
125. 含文字图片的处理方案
126. InstructGPT和ChatGPT的关键技术（SFT到RLHF）
127. LoRA原理
128. 评估指标选择（是否仅用AUC）
129. FP32/FP16区别及混合精度原理
130. 论文创新点和贡献
131. 模型合并后的推理速度变化
132. 论文相关问题
133. Python基础（dict有序性、is与==区别）
134. Transformer结构介绍
135. 二叉树中序遍历实现
136. 深层大模型的梯度消失预防方法
137. PPO相关模型介绍（承认无训练经验）
138. 印象最深的项目经历（问题及解决方案）
139. encoder和decoder中掩码的区别
140. decoder-only和encoder各自优势
141. Faiss原理、索引方式及业务应用
142. Recall指标的选择原因
143. 对MMOE的理解
144. LLaMa与Qwen系列的区别
145. prompt设计方法
146. 特征选择依据
147. 二叉搜索树数量计算（动态规划）
148. 用户隐私数据处理方法
149. 知识图谱与大模型结合方案
150. 二分查找算法题
151. 选择CTR和CVR建模的原因
152. BERT与GPT的区别
153. YOLOv1/v2/v3细节记忆
154. 代码细节和attention机制原理询问
155. 对比MMOE、PLE和AdaTT的改进及原因
156. CV实习项目的实现流程
157. 邻接链表类实现（含插入节点/边和深拷贝）
158. 百度实习部门及未留任原因
159. AdaTT的定义
160. 大模型项目流程探讨（简要）
161. 代码能力考查（C++/Python/SQL）
162. C++编码经验
163. Transformer架构介绍
164. 算法题：下一个排列（LeetCode 31题）
165. DDIM与DDPM原理及区别
166. 项目背景/目标/流程/数据处理/标签对齐方式
167. ChatGPT相比GPT-3的性能提升点
168. 算法：算术平方根计算（精度0.00001）
169. 多模态大模型训练经验
170. Paddle使用经验
171. 准确率和召回率定义
172. 特征重要性分析及额外处理
173. RAG相关项目介绍及细节回答
174. "特征比模型更重要"观点讨论
175. 业务/技术栈/后续流程询问
176. PyFlask使用经验
177. 召回与排序方法及特征构造思路
178. OCR文字提取及嵌入向量对齐
179. 申请百度需熟练掌握螺旋矩阵题型变种
```

# 面试经验

```markdown
1. 面试中表达不够流畅，但面试官耐心解答和纠正
2. 最近有朋友面试百度大模型算法岗，反馈面试官专业度高，难度和范围都很大，三轮面试逐步深入
3. 本人研二在读（本硕985），无发表论文，有3个本科和教研室的NLP项目、1个多模态项目和1个RAG项目，正在找第一份实习
4. 面试官评价学习能力不错但需要沉淀，最终给予通过
5. 意外通过第三轮后，第四轮是部门主管面，讨论未来业务方向后口头通知录用
6. 面试官跳过自我介绍环节，表示已了解背景
7. 后续可能还有1-2轮技术面试
8. HR在首面后40分钟内电话预约二面时间，效率很高
9. 实习经历以聊天形式询问，未专门考察理论知识
10. 时隔一年多的首次面试，因准备不足感到紧张，面试官态度友好，计划周末复盘
11. 二面次日即安排三面
12. 因提前准备充分，最终通过面试
13. 仅口头说明代码修改思路未实际编写，仍获通过
14. 原以为三面是HR面，实际仍为技术面
15. 部分回答不理想但最后收到合作邀请，不确定录用概率
16. 某些问题讨论时间较长，自我感觉回答欠佳
17. 因一面表现优秀，当晚即安排二面
18. 所有问题都作答但不确定回答质量
19. 年轻面试官提问犀利，重点考察模型理解，部分问题思考较久，感到压力
20. 面试官询问现有offer情况及春招进程
21. 自我介绍简短，按简历重点陈述
22. 整体流程顺畅，面试官友好，体验良好
23. 二面因技术故障转为做题，业务问题非常细节，结束后半小时通知通过并沟通入职时间
24. 上半年面试体验最佳的一次
25. 中年面试官态度温和，主要深挖项目细节
26. 面试中获得多次正面反馈
27. 面试官肯定基础知识但指出算法需加强，最终未通过
28. 面试以场景题为主，未考八股文
29. 前两面结果次日即出，三面两天无消息后主动询问HR才知通过
30. 要求详细介绍百度实习经历
31. 询问职业选择动机
32. 大厂面试问题高度相似，需重点准备高频考点
33. 要求选择一段经历详细说明
34. 强调避免关键问题失误
35. 必须熟悉自身工作场景细节
36. 强调要掌握简历项目的场景规模、模型规模和数据规模
```
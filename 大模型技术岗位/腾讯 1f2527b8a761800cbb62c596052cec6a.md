# 腾讯

创建时间: 2025年5月14日 00:15

# 面试题目

```markdown
1. 你平时是怎么学习的？有什么自学方法？  
2. 你未来想从事哪个方向的工作？  
3. 简历上只写了一段实习经历和一个天池比赛经历，但多模态方向的比赛没被问到。  
4. 第四轮面试主要考察思维能力和基础知识。  
5. 一面问了很多基础理论题。  
6. RAG相比微调解决了什么问题？  
7. 你看过Meta的Metatron和DeepSpeed的源码吗？包括ZeRO优化是怎么实现的？  
8. 如果计算资源充足，为什么不做全参数微调？LoRA的参数为什么这么选？  
9. 如果要让大模型做数据分析，你会怎么设计？  
10. 能解释下C++虚函数的作用吗？  
11. 你怎么评估大模型输出的质量？如果结果不满意要怎么优化？  
12. 你用的什么embedding模型和数据库？  
13. 你用过哪些大模型？能简单说说它们的结构吗？  
14. 怎么估算训练和推理需要的显存？  
15. 大模型训练的基本流程是什么？  
16. 了解哪些Transformer推理加速技术？比如vLLM。  
17. 讲实习项目时，被问到多路召回怎么合并、怎么做小流量AB测试，以及怎么确定各路的召回比例。  
18. 项目里部署大模型遇到哪些性能问题？怎么解决的？  
19. BERT和LLaMA的位置编码有什么区别？  
20. 详细说明Transformer的前向计算过程，包括每一步的输入输出维度。  
21. 为什么会发生梯度消失和爆炸？怎么解决？  
22. 说说ReLU和SiLU这些激活函数的特点。  
23. 你有做过意图识别和代码生成吗？（这个岗位是大模型算法岗）  
24. FlashAttention是怎么优化计算的？  
25. 写个非递归的快排实现（能跑通测试用例）。  
26. 你觉得大模型比传统模型强在哪里？  
27. encoder-only、decoder-only和encoder-decoder架构各适合什么任务？  
28. Python里__init__和__new__有什么区别？  
29. 为什么思维链（COT）提示有效？大模型相比传统模型有什么优势？  
30. 如果用大模型做分类效果不好，可以怎么优化？类别很多还不均衡时怎么办？  
31. 有监督微调有哪些方式？比如全参数微调和LoRA。  
32. 你之前用BERT做文本分类，为什么不用大模型？用大模型怎么做分类？  
33. 你处理过音频数据吗？了解语音转文本模型吗？  
34. 常见的大模型有哪些？它们做了哪些改进？  
35. 目前大模型有哪些实际应用场景？  
36. 在项目或比赛中遇到过什么困难？怎么解决的？  
37. 解释下多头注意力机制，和普通注意力有什么区别？  
38. 对比过LoRA微调和全参数微调的效果吗？微调和推理各需要多少显存？  
39. 调大模型微调时要注意哪些参数？  
40. 全参数微调和LoRA微调在资源和效果上有什么区别？  
41. 大模型训练分哪些阶段？Prompt和SFT各适合什么场景？  
42. 有监督微调的数据集要多大？数据怎么配比？  
43. 怎么微调embedding模型？负样本怎么选？RAG里还有哪些微调方法？  
44. 介绍下Transformer的几种变体，比如encoder-only和decoder-only，它们有什么区别？  
45. 有8个球，1个比较轻，最少称几次能找出来？  
46. 为什么大模型用LayerNorm而不用BatchNorm？有什么区别？  
47. 深入讨论RLHF对齐的问题。  
48. 计算CNN的计算量：输入224×224×3，卷积核3×3×64，默认stride和padding。  
49. 知识库一般多大？怎么组织结构的？  
50. text2sql任务用什么指标评估？现在主流方法有哪些？（项目里做过这个）  
51. 详细说说你的实习经历和做过的项目。  
52. 在RAG里，除了用query+文档片段生成答案，还有什么更好的方法？  
53. 大模型的基础知识。  
54. 怎么计算训练模型需要的显存？  
55. Python能实现函数重载吗？  
56. 写个合并区间的算法题（LeetCode）。  
57. 训练大模型用了多少资源？用什么并行方式？训练多久？调了哪些参数？遇到什么问题？  
58. 全参数微调会有什么问题？怎么缓解灾难性遗忘？  
59. 介绍一下Transformer的结构。  
60. TCP和UDP有什么区别？Linux怎么查看正在运行的任务？怎么强制结束任务？  
61. 了解DeepSeek吗？说说它的一个创新点。  
62. 介绍参数高效微调方法（PEFT）。  
63. 从你的工作经历引出RLHF相关的内容。  
64. 你们团队怎么协作的？遇到过什么问题？怎么解决的？
```

# 面试经验

```markdown
1. 你能接受怎样的工作时间安排？
2. 二面是总监面试，整体氛围比较紧张。因为迟到了几分钟，刚开始态度有些严肃。面试中对简历进行了深入询问，特别区分了"熟悉"和"了解"的程度，感觉像是一次全面的指导。
3. 二面是总监面，主要聊个人经历，也穿插了一些轻松话题。面试官很友善，没有施加太大压力。总监面一般不会考代码题。我参加过三个不同事业群的总监面，都没遇到代码题，也很少问八股文。
4. 一面比较常规，面试官主要做数据挖掘，对大模型不太了解。二面问题比较宏观，技术问题不多。可能是新业务急需人手，招聘流程走得很快。但考虑到实习转正和秋招难以兼顾，最后选择了秋招，所以没去成。
5. 训练营学员背景：211本硕，发表过一篇B类二作论文，之前没有实习经历，在辅导中补充了两个大模型项目经验。
6. 后面问了些闲聊性质的问题，整体感觉不错，但二面体验不太好。
7. 你本科和研究生专业都不是计算机或AI相关，现在却做这个方向，你是怎么完成这个转变的？
8. 你觉得自己的优势在哪里？
9. 投的是推荐算法岗，简历可能被其他部门看中了。面试问题多是多模态相关，都比较基础，整体难度不大。代码题直接来自LeetCode。现在状态已进入复试，大概3天更新一次，还没约具体面试时间。听说二面难度会大很多，需要好好准备。
10. 你家乡在xx，能接受去外地工作吗？对异地工作怎么看？
11. 最后只问了是日常实习还是暑期实习。
12. 211本硕，NLP专业，无论文，有实习经历（老板资源有限且管理宽松），本科参加过ACM拿过铜牌，面的PCG日常实习。
13. 二面很快就结束了，一天后流程终止，继续努力。
14. 面试官看起来不太感兴趣，只是象征性地问了实习时间和导师意见。
15. 面试部门主要做对齐相关工作。
16. 虽然回答得不太好，但幸运地收到了复试通知。
17. 你未来的职业规划是什么？有读博的打算吗？
18. 研二，本硕都是985，无论文，有两段实习经历。
19. 面试官直接进入主题，他没开摄像头，我提前开了，也不好意思关。
20. 腾讯HR面试比较正规，一般要求开摄像头，时长大概15到30分钟。
```